{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial - Advantage Actor Critic (A2C).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOerJxVFIaozWjxy5taLfea",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rlberry-py/tutorials/blob/main/A2C/Tutorial_Advantage_Actor_Critic_(A2C).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRvfou6G9RGn"
      },
      "source": [
        "# Tutorial - Advantage Actor Critic (A2C)\n",
        "\n",
        "A2C keeps two neural networks:\n",
        "*   One network with paramemeters $\\theta$ to represent the policy $\\pi_\\theta$.\n",
        "*   One network with parameters $\\omega$ to represent a value function $V_\\omega$, that approximates $V^{\\pi_\\theta}$\n",
        "\n",
        "\n",
        "At each iteration, A2C collects $M$ transitions $(s_i, a_i, r_i, s_i')_{i=1}^M$ by following the policy $\\pi_\\theta$. If a terminal state is reached, we simply go back to the initial state and continue to play $\\pi_\\theta$ until we gather the $M$ transitions.\n",
        "\n",
        "Consider the following quantities, defined based on the collected transitions:\n",
        "\n",
        "$$\n",
        "\\widehat{V}(s_i) = \\widehat{Q}(s_i, a_i) = \\sum_{t=i}^{\\tau_i \\wedge M} \\gamma^{t-i} r_t + \\gamma^{M-i+1} V_\\omega(s_M')\\mathbb{I}\\{\\tau_i>M\\}\n",
        "$$\n",
        "\n",
        "where and $\\tau_i = \\min\\{t\\geq i: s_i' \\text{ is a terminal state}\\}$, and \n",
        "\n",
        "$$\n",
        "\\mathbf{A}_\\omega(s_i, a_i) = \\widehat{Q}(s_i, a_i) -  V_\\omega(s_i)  \n",
        "$$\n",
        "\n",
        "\n",
        "A2C then takes a gradient step to minimize the policy \"loss\" (keeping $\\omega$ fixed):\n",
        "\n",
        "$$\n",
        "L_\\pi(\\theta) =\n",
        "-\\frac{1}{M} \\sum_{i=1}^M \\mathbf{A}_\\omega(s_i, a_i) \\log \\pi_\\theta(a_i|s_i)\n",
        "- \\frac{\\alpha}{M}\\sum_{i=1}^M \\sum_a  \\pi(a|s_i) \\log \\frac{1}{\\pi(a|s_i)}\n",
        "$$\n",
        "\n",
        "and a gradient step to minimize the value loss (keeping $\\theta$ fixed):\n",
        "\n",
        "$$\n",
        "L_v(\\omega) = \\frac{1}{M} \\sum_{i=1}^M \\left( \\widehat{V}(s_i) - V_\\omega(s_i)   \\right)^2\n",
        "$$\n",
        " \n",
        "\n",
        "\n",
        "# Reminders\n",
        "\n",
        "\n",
        "Objective function:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\n",
        "\\left[ \n",
        "  \\sum_{t=0}^\\infty \\gamma^t r(S_t, A_t)\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "Policy gradient:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta)= \\mathbb{E}_{\\pi_\\theta}\n",
        "\\left[ \n",
        "  \\sum_{t=0}^\\infty \\gamma^t A^{\\pi_\\theta}(S_t, A_t) \n",
        "  \\nabla_\\theta \\log \\pi_\\theta(A_t|S_t)\n",
        "\\right]\n",
        "$$\n",
        "where $A^{\\pi_\\theta}(s, a) = Q^{\\pi_\\theta}(s, a) - V^{\\pi_\\theta}(s) $ is the advantage function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er4wbIih9e24"
      },
      "source": [
        "# Colab setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O12jMLD29DAU",
        "outputId": "37a4b59a-2b5d-44f4-da53-51fd84d77c3f"
      },
      "source": [
        "# After installing, restart the kernel\n",
        "\n",
        "# install rlberry library\n",
        "!git clone https://github.com/rlberry-py/rlberry.git \n",
        "!cd rlberry && git pull && pip install -e .[full] > /dev/null 2>&1\n",
        "!pip install ffmpeg-python > /dev/null 2>&1\n",
        "\n",
        "# gym\n",
        "!pip install 'gym[all]' > /dev/null 2>&1\n",
        "\n",
        "# packages required to show video\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "# ask to restart runtime\n",
        "print(\"\")\n",
        "print(\" ~~~  Libraries installed, please restart the runtime! ~~~ \")\n",
        "print(\"\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rlberry'...\n",
            "remote: Enumerating objects: 472, done.\u001b[K\n",
            "remote: Counting objects: 100% (472/472), done.\u001b[K\n",
            "remote: Compressing objects: 100% (292/292), done.\u001b[K\n",
            "remote: Total 3541 (delta 283), reused 326 (delta 177), pack-reused 3069\u001b[K\n",
            "Receiving objects: 100% (3541/3541), 886.51 KiB | 9.85 MiB/s, done.\n",
            "Resolving deltas: 100% (2277/2277), done.\n",
            "Already up to date.\n",
            "\n",
            " ~~~  Libraries installed, please restart the runtime! ~~~ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKOp4h0Oe9-X"
      },
      "source": [
        "import gym\r\n",
        "from gym import logger as gymlogger\r\n",
        "from gym.wrappers import Monitor\r\n",
        "gymlogger.set_level(40)  # error only\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F \r\n",
        "from torch import optim\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "# for videos\r\n",
        "import rlberry.colab_utils.display_setup\r\n",
        "from rlberry.colab_utils.display_setup import show_video"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MESFRbWdfA6P"
      },
      "source": [
        "class ActorNetwork(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "     This network represents the policy\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, input_size, hidden_size, action_size):\r\n",
        "        super(ActorNetwork, self).__init__()\r\n",
        "        self.n_actions = action_size\r\n",
        "        self.dim_observation = input_size\r\n",
        "        \r\n",
        "        self.net = nn.Sequential(\r\n",
        "            nn.Linear(in_features=self.dim_observation, out_features=hidden_size),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(in_features=hidden_size, out_features=self.n_actions),\r\n",
        "            nn.Softmax(dim=-1)\r\n",
        "        )\r\n",
        "        \r\n",
        "    def policy(self, state):\r\n",
        "        state = torch.tensor(state, dtype=torch.float)\r\n",
        "        return self.net(state)\r\n",
        "    \r\n",
        "    def sample_action(self, state):\r\n",
        "        state = torch.tensor(state, dtype=torch.float)\r\n",
        "        action = torch.multinomial(self.policy(state), 1)\r\n",
        "        return action.item()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_DHHAQNfD7Z"
      },
      "source": [
        "class ValueNetwork(nn.Module):\r\n",
        "  \"\"\"\r\n",
        "   This class represents the value function\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  def __init__(self, input_size, hidden_size, output_size):\r\n",
        "      super(ValueNetwork, self).__init__()\r\n",
        "      self.fc1 = nn.Linear(input_size, hidden_size)\r\n",
        "      self.fc2 = nn.Linear(hidden_size, hidden_size)\r\n",
        "      self.fc3 = nn.Linear(hidden_size, output_size)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "      out = F.relu(self.fc1(x))\r\n",
        "      out = F.relu(self.fc2(out))\r\n",
        "      out = self.fc3(out)\r\n",
        "      return out\r\n",
        "  \r\n",
        "  def value(self, state):\r\n",
        "      state = torch.tensor(state, dtype=torch.float)\r\n",
        "      return self.forward(state)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ry-b3HgfGx5"
      },
      "source": [
        "# You can select your environment here\r\n",
        "env_id = 'CartPole-v1'  # @param [\"CartPole-v1\", \"LunarLander-v2\", \"MountainCar-v0\"]\r\n",
        "env = gym.make(env_id)\r\n",
        "eval_env = gym.make(env_id) # environment to evaluate the policy"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h65dXIY5fMZg"
      },
      "source": [
        "# Define you networks\r\n",
        "value_network = ValueNetwork(env.observation_space.shape[0], 16, 1)\r\n",
        "actor_network = ActorNetwork(env.observation_space.shape[0], 16, env.action_space.n)\r\n",
        "print(value_network)\r\n",
        "print(actor_network)\r\n",
        "\r\n",
        "# Define your optimizers\r\n",
        "value_network_optimizer = torch.optim.RMSprop(value_network.parameters(), lr=0.01)\r\n",
        "actor_network_optimizer = torch.optim.RMSprop(actor_network.parameters(), lr=0.01)\r\n",
        "\r\n",
        "# --------------------------------------------------------------\r\n",
        "# Parameters\r\n",
        "# --------------------------------------------------------------\r\n",
        "num_iterations = 300     # Number of iterations\r\n",
        "batch_size = 512         # How many samples to collect (value of M)\r\n",
        "gamma = 1                # Discount factor\r\n",
        "alpha = 0.001            # Entropy term coefficient\r\n",
        "reward_threshold = 495   # Stop training when the policy achieves this amound of rewards\r\n",
        "\r\n",
        "\r\n",
        "# --------------------------------------------------------------\r\n",
        "# Train\r\n",
        "# --------------------------------------------------------------\r\n",
        "for iteration in range(num_iterations):\r\n",
        "    # Initialize batch storage\r\n",
        "    states = np.empty((batch_size,) + env.observation_space.shape, dtype=np.float)        # shape (batch_size, state_dim)\r\n",
        "    rewards = np.empty((batch_size,), dtype=np.float)                                     # shape (batch_size, )                                 \r\n",
        "    next_states = np.empty((batch_size,) + env.observation_space.shape, dtype=np.float)   # shape (batch_size, state_dim)\r\n",
        "    dones = np.empty((batch_size,), dtype=np.bool)                                        # shape (batch_size, ) \r\n",
        "    proba = torch.empty((batch_size,), dtype=np.float)                                    # shape (batch_size, ), store pi(a_t|s_t)\r\n",
        "    next_value = 0                               # \r\n",
        "  \r\n",
        "    # Intialize environment\r\n",
        "    state = env.reset()\r\n",
        "\r\n",
        "  # Generate batch\r\n",
        "    for i in range(batch_size):\r\n",
        "        action = actor_network.sample_action(state)\r\n",
        "        next_state, reward, done, _ = env.step(action)\r\n",
        "\r\n",
        "        states[i] = # ...\r\n",
        "        rewards[i] = # ...\r\n",
        "        next_states[i] = # ...\r\n",
        "        dones[i] = # ...\r\n",
        "        proba[i] = # ...\r\n",
        "\r\n",
        "        state = next_state\r\n",
        "        if done:\r\n",
        "          state = env.reset()\r\n",
        "\r\n",
        "    if not done:\r\n",
        "        next_value = value_network.value(next_states[-1]).detach().numpy()[0]\r\n",
        "\r\n",
        "    # compute returns (without bootstrapping)\r\n",
        "    returns = np.zeros((batch_size,), dtype=np.float)\r\n",
        "    T = batch_size\r\n",
        "    for j in range(T):\r\n",
        "        returns[T-j-1] = rewards[T-j-1]\r\n",
        "        if j > 0:\r\n",
        "            returns[T-j-1] += gamma * returns[T-j] * (1 - dones[T-j])\r\n",
        "        else:\r\n",
        "            returns[T-j-1] += gamma * next_value\r\n",
        "\r\n",
        "    # compute advantage\r\n",
        "    values = value_network.value(states)\r\n",
        "    advantages = # ...\r\n",
        "\r\n",
        "    # Compute MSE (Value loss)\r\n",
        "    value_network_optimizer.zero_grad()\r\n",
        "    loss_value = # ...\r\n",
        "    loss_value.backward()\r\n",
        "    value_network_optimizer.step()\r\n",
        "\r\n",
        "    # Compute entropy term\r\n",
        "    dist = actor_network.policy(states)\r\n",
        "    entropy_term = -(dist*dist.log()).sum(-1).mean()\r\n",
        "\r\n",
        "    # Compute policy loss\r\n",
        "    actor_network_optimizer.zero_grad()\r\n",
        "    loss_policy = # ...\r\n",
        "    loss_policy += -alpha * entropy_term\r\n",
        "    loss_policy.backward()\r\n",
        "    actor_network_optimizer.step()\r\n",
        "\r\n",
        "    if( (iteration+1)%10 == 0 ):\r\n",
        "        eval_rewards = np.zeros(5)\r\n",
        "        for sim in range(5):\r\n",
        "            eval_done = False\r\n",
        "            eval_state = eval_env.reset()\r\n",
        "            while not eval_done:\r\n",
        "                eval_action = actor_network.sample_action(eval_state)\r\n",
        "                eval_next_state, eval_reward, eval_done, _ = eval_env.step(eval_action)\r\n",
        "                eval_rewards[sim] += eval_reward\r\n",
        "                eval_state = eval_next_state\r\n",
        "        print(\"Iteration = {}, loss_value = {:0.3f}, loss_policy = {:0.3f}, rewards = {:0.2f}\"\r\n",
        "              .format(iteration +1, loss_value.item(), loss_policy.item(), eval_rewards.mean()))\r\n",
        "        if (eval_rewards.mean() > reward_threshold):\r\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPzvAqDVhc_K"
      },
      "source": [
        "env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)\r\n",
        "for episode in range(1):\r\n",
        "    done = False\r\n",
        "    state = env.reset()\r\n",
        "    while not done:\r\n",
        "        action = actor_network.sample_action(state)\r\n",
        "        state, reward, done, info = env.step(action)\r\n",
        "env.close()\r\n",
        "show_video(directory=\"./gym-results\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNqnseJtlU87"
      },
      "source": [
        "# Test other environments!\r\n",
        "\r\n",
        "Try some other environments available in OpenAI gym ([link](https://gym.openai.com/envs/#classic_control)). Suggestion: use `classic control` or `Box2D` environments."
      ]
    }
  ]
}