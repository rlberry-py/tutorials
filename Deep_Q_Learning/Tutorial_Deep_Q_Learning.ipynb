{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial_Deep_Q_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMSYGuoVaPQMZ/E6TpcLRNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rlberry-py/tutorials/blob/main/Deep_Q_Learning/Tutorial_Deep_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j_no2BuvPUE"
      },
      "source": [
        "# Tutorial - Deep Q-Learning \n",
        "\n",
        "Deep Q-Learning used a neural network to approximate $Q$ functions. Hence, we usually refer to this algorithm as DQN (for *deep Q network*).\n",
        "\n",
        "The parameters of the neural network are denoted by $\\theta$. \n",
        "*   As input, the network takes a state $s$,\n",
        "*   As output, the network returns $Q(s, a, \\theta)$, the value of each action $a$ in state $s$, according to the parameters $\\theta$.\n",
        "\n",
        "\n",
        "The goal of Deep Q-Learning is to learn the parameters $\\theta$ so that $Q(s, a, \\theta)$ approximates well the optimal $Q$-function $Q^*(s, a)$. \n",
        "\n",
        "In addition to the network with parameters $\\theta$, the algorithm keeps another network with the same architecture and parameters $\\theta^-$, called **target network**.\n",
        "\n",
        "The algorithm works as follows:\n",
        "\n",
        "1.   At each time $t$, the agent is in state $s_t$ and has observed the transitions $(s_i, a_i, r_i, s_i')_{i=1}^{t-1}$, which are stored in a **replay buffer**.\n",
        "\n",
        "2.  Choose action $a_t = \\arg\\max_a Q(s_t, a)$ with probability $1-\\varepsilon_t$, and $a_t$=random action with probability $\\varepsilon_t$. \n",
        "\n",
        "3. Take action $a_t$, observe reward $r_t$ and next state $s_t'$.\n",
        "\n",
        "4. Add transition $(s_t, a_t, r_t, s_t')$ to the **replay buffer**.\n",
        "\n",
        "4.  Sample a minibatch $\\mathcal{B}$ containing $B$ transitions from the replay buffer. Using this minibatch, we define the loss:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\sum_{(s_i, a_i, r_i, s_i') \\in \\mathcal{B}}\n",
        "\\left[\n",
        "Q(s_i, a_i, \\theta) -  y_i\n",
        "\\right]^2\n",
        "$$\n",
        "where the $y_i$ are the **targets** computed with the **target network** $\\theta^-$:\n",
        "\n",
        "$$\n",
        "y_i = r_i + \\gamma \\max_{a'} Q(s_i', a', \\theta^-).\n",
        "$$\n",
        "\n",
        "5. Update the parameters $\\theta$ to minimize the loss, e.g., with gradient descent (**keeping $\\theta^-$ fixed**): \n",
        "$$\n",
        "\\theta \\gets \\theta + \\eta \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "where $\\eta$ is the optimization learning rate. \n",
        "\n",
        "6. Every $N$ transitions ($t\\mod N$ = 0), update target parameters: $\\theta^- \\gets \\theta$.\n",
        "\n",
        "7. $t \\gets t+1$. Stop if $t = T$, otherwise go to step 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhKHif__t9OD"
      },
      "source": [
        "# Colab setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aylqy_sDqebM",
        "outputId": "3c08290e-3847-4ebd-e966-1a6f731f6bc7"
      },
      "source": [
        "# After installing, restart the kernel\n",
        "\n",
        "# install rlberry library\n",
        "!git clone https://github.com/rlberry-py/rlberry.git \n",
        "!cd rlberry && git pull && pip install -e .[full] > /dev/null 2>&1\n",
        "!pip install ffmpeg-python > /dev/null 2>&1\n",
        "\n",
        "# packages required to show video\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "# ask to restart runtime\n",
        "print(\"\")\n",
        "print(\" ~~~  Libraries installed, please restart the runtime! ~~~ \")\n",
        "print(\"\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rlberry'...\n",
            "remote: Enumerating objects: 162, done.\u001b[K\n",
            "remote: Counting objects: 100% (162/162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 3231 (delta 80), reused 100 (delta 45), pack-reused 3069\u001b[K\n",
            "Receiving objects: 100% (3231/3231), 821.81 KiB | 13.04 MiB/s, done.\n",
            "Resolving deltas: 100% (2074/2074), done.\n",
            "Already up to date.\n",
            "\n",
            " ~~~  Libraries installed, please restart the runtime! ~~~ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWBRfwosfA9f"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "# Random number generator\n",
        "import rlberry.seeding as seeding \n",
        "rng = seeding.get_rng()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "528oqsgefIFl"
      },
      "source": [
        "# 1. Define the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtExtR4dfMbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6166f6fa-d6dc-4663-a05f-6798bcebdad9"
      },
      "source": [
        "# Environment\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# Discount factor\n",
        "GAMMA = 0.99\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 256\n",
        "# Capacity of the replay buffer\n",
        "BUFFER_CAPACITY = 10000\n",
        "# Update target net every ... episodes\n",
        "UPDATE_TARGET_EVERY = 20\n",
        "\n",
        "# Initial value of epsilon\n",
        "EPSILON_START = 1.0\n",
        "# Parameter to decrease epsilon\n",
        "DECREASE_EPSILON = 200\n",
        "# Minimum value of epislon\n",
        "EPSILON_MIN = 0.05\n",
        "\n",
        "# Number of training episodes\n",
        "N_EPISODES = 200\n",
        "\n",
        "# Learning rate\n",
        "LEARNING_RATE = 0.1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Making new env: CartPole-v0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g16Je-dhM2Q"
      },
      "source": [
        "# 2. Define the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvh82br9hMNt"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (state, action, reward, next_state)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return rng.choice(self.memory, batch_size).tolist()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# create instance of replay buffer\n",
        "replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCc9WZppi92W"
      },
      "source": [
        "# 3. Define the neural network architecture, objective and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdNz3Jrwi9iS"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic neural net.\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI9hFJ28jLZ_"
      },
      "source": [
        "# create network and target network\n",
        "hidden_size = 128\n",
        "obs_size = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "q_net = Net(obs_size, hidden_size, n_actions)\n",
        "target_net = Net(obs_size, hidden_size, n_actions)\n",
        "\n",
        "# objective and optimizer\n",
        "objective = nn.MSELoss()\n",
        "optimizer = optim.Adam(params=q_net.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnR8nfoSjZjL"
      },
      "source": [
        "# 4. Implement Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6fT8cKdjmTZ"
      },
      "source": [
        "#\n",
        "#  Some useful functions\n",
        "#\n",
        "\n",
        "def get_q(states):\n",
        "    \"\"\"\n",
        "    Compute Q function for a list of states\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        states_v = torch.FloatTensor([states])\n",
        "        output = q_net.forward(states_v).data.numpy()  # shape (1, len(states), n_actions)\n",
        "    return output[0, :, :]  # shape (len(states), n_actions)\n",
        "\n",
        "def eval_dqn(n_sim=5):\n",
        "    \"\"\"\n",
        "    ** TO BE IMPLEMENTED **\n",
        "    \n",
        "    Monte Carlo evaluation of DQN agent.\n",
        "\n",
        "    Repeat n_sim times:\n",
        "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
        "        * Compute the sum of rewards in this episode\n",
        "        * Store the sum of rewards in the episode_rewards array.\n",
        "    \"\"\"\n",
        "    env_copy = deepcopy(env)\n",
        "    episode_rewards = np.zeros(n_sim)\n",
        "    return episode_rewards"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMspDNntkIoe"
      },
      "source": [
        "def choose_action(state, epsilon):\n",
        "    \"\"\"\n",
        "    ** TO BE IMPLEMENTED **\n",
        "    \n",
        "    Return action according to an epsilon-greedy exploration policy\n",
        "    \"\"\"\n",
        "    return 0\n",
        "    \n",
        "\n",
        "def update(state, action, reward, next_state, done):\n",
        "    \"\"\"\n",
        "    ** TO BE COMPLETED **\n",
        "    \"\"\"\n",
        "    \n",
        "    # add data to replay buffer\n",
        "    if done:\n",
        "        next_state = None\n",
        "    replay_buffer.push(state, action, reward, next_state)\n",
        "    \n",
        "    if len(replay_buffer) < BATCH_SIZE:\n",
        "        return np.inf\n",
        "    \n",
        "    # get batch\n",
        "    transitions = replay_buffer.sample(BATCH_SIZE)\n",
        "    \n",
        "    # Compute loss - TO BE IMPLEMENTED!\n",
        "    values  = torch.zeros(BATCH_SIZE)   # to be computed using batch\n",
        "    targets = torch.zeros(BATCH_SIZE)   # to be computed using batch\n",
        "    loss = objective(values, targets)\n",
        "     \n",
        "    # Optimize the model - UNCOMMENT!\n",
        "#     optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "    \n",
        "    return loss.data.numpy()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIhpKPhkkU4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f11a2d6-4303-44e8-938d-6b1ddf26927e"
      },
      "source": [
        "\n",
        "#\n",
        "# Train\n",
        "# \n",
        "\n",
        "EVAL_EVERY = 5\n",
        "REWARD_THRESHOLD = 199\n",
        "\n",
        "def train():\n",
        "    state = env.reset()\n",
        "    epsilon = EPSILON_START\n",
        "    ep = 0\n",
        "    total_time = 0\n",
        "    while ep < N_EPISODES:\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # take action and update replay buffer and networks\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        loss = update(state, action, reward, next_state, done)\n",
        "\n",
        "        # update state\n",
        "        state = next_state\n",
        "\n",
        "        # end episode if done\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            ep   += 1\n",
        "            if ( (ep+1)% EVAL_EVERY == 0):\n",
        "                rewards = eval_dqn()\n",
        "                print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n",
        "                if np.mean(rewards) >= REWARD_THRESHOLD:\n",
        "                    break\n",
        "\n",
        "            # update target network\n",
        "            if ep % UPDATE_TARGET_EVERY == 0:\n",
        "                target_net.load_state_dict(q_net.state_dict())\n",
        "            # decrease epsilon\n",
        "            epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN) * \\\n",
        "                            np.exp(-1. * ep / DECREASE_EPSILON )    \n",
        "\n",
        "        total_time += 1\n",
        "\n",
        "# Run the training loop\n",
        "train()\n",
        "\n",
        "# Evaluate the final policy\n",
        "rewards = eval_dqn(20)\n",
        "print(\"\")\n",
        "print(\"mean reward after training = \", np.mean(rewards))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode = 5 , reward =  0.0\n",
            "episode = 10 , reward =  0.0\n",
            "episode = 15 , reward =  0.0\n",
            "episode = 20 , reward =  0.0\n",
            "episode = 25 , reward =  0.0\n",
            "episode = 30 , reward =  0.0\n",
            "episode = 35 , reward =  0.0\n",
            "episode = 40 , reward =  0.0\n",
            "episode = 45 , reward =  0.0\n",
            "episode = 50 , reward =  0.0\n",
            "episode = 55 , reward =  0.0\n",
            "episode = 60 , reward =  0.0\n",
            "episode = 65 , reward =  0.0\n",
            "episode = 70 , reward =  0.0\n",
            "episode = 75 , reward =  0.0\n",
            "episode = 80 , reward =  0.0\n",
            "episode = 85 , reward =  0.0\n",
            "episode = 90 , reward =  0.0\n",
            "episode = 95 , reward =  0.0\n",
            "episode = 100 , reward =  0.0\n",
            "episode = 105 , reward =  0.0\n",
            "episode = 110 , reward =  0.0\n",
            "episode = 115 , reward =  0.0\n",
            "episode = 120 , reward =  0.0\n",
            "episode = 125 , reward =  0.0\n",
            "episode = 130 , reward =  0.0\n",
            "episode = 135 , reward =  0.0\n",
            "episode = 140 , reward =  0.0\n",
            "episode = 145 , reward =  0.0\n",
            "episode = 150 , reward =  0.0\n",
            "episode = 155 , reward =  0.0\n",
            "episode = 160 , reward =  0.0\n",
            "episode = 165 , reward =  0.0\n",
            "episode = 170 , reward =  0.0\n",
            "episode = 175 , reward =  0.0\n",
            "episode = 180 , reward =  0.0\n",
            "episode = 185 , reward =  0.0\n",
            "episode = 190 , reward =  0.0\n",
            "episode = 195 , reward =  0.0\n",
            "episode = 200 , reward =  0.0\n",
            "\n",
            "mean reward after training =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8QZwuvjgrMm"
      },
      "source": [
        "# Visualize the DQN policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "FGcGwOcEfzPz",
        "outputId": "1c591794-5f9c-4c4d-916a-86c158671e3d"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path\n",
        "import base64\n",
        "\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "    html = []\n",
        "    for mp4 in Path(\"videos\").glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay \n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "    \n",
        "env = Monitor(env, './videos', force=True, video_callable=lambda episode: True)\n",
        "\n",
        "for episode in range(1):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    while not done:\n",
        "        action = choose_action(state, 0.0)\n",
        "        state, reward, done, info = env.step(action)\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Clearing 2 monitor files from previous run (because force=True was provided)\n",
            "INFO: Starting new video recorder writing to /content/videos/openaigym.video.1.491.video000000.mp4\n",
            "INFO: Finished writing results. You can upload them to the scoreboard via gym.upload('/content/videos')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"videos/openaigym.video.1.491.video000000.mp4\" autoplay \n",
              "                      loop controls style=\"height: 400px;\">\n",
              "                      <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "                 </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}