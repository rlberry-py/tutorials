{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution_Tutorial_Deep_Q_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObnnsRGNvUSOnBQsyvncOn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rlberry-py/tutorials/blob/main/Deep_Q_Learning/Solution_Tutorial_Deep_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j_no2BuvPUE"
      },
      "source": [
        "# Tutorial - Deep Q-Learning \n",
        "\n",
        "Deep Q-Learning uses a neural network to approximate $Q$ functions. Hence, we usually refer to this algorithm as DQN (for *deep Q network*).\n",
        "\n",
        "The parameters of the neural network are denoted by $\\theta$. \n",
        "*   As input, the network takes a state $s$,\n",
        "*   As output, the network returns $Q(s, a, \\theta)$, the value of each action $a$ in state $s$, according to the parameters $\\theta$.\n",
        "\n",
        "\n",
        "The goal of Deep Q-Learning is to learn the parameters $\\theta$ so that $Q(s, a, \\theta)$ approximates well the optimal $Q$-function $Q^*(s, a)$. \n",
        "\n",
        "In addition to the network with parameters $\\theta$, the algorithm keeps another network with the same architecture and parameters $\\theta^-$, called **target network**.\n",
        "\n",
        "The algorithm works as follows:\n",
        "\n",
        "1.   At each time $t$, the agent is in state $s_t$ and has observed the transitions $(s_i, a_i, r_i, s_i')_{i=1}^{t-1}$, which are stored in a **replay buffer**.\n",
        "\n",
        "2.  Choose action $a_t = \\arg\\max_a Q(s_t, a)$ with probability $1-\\varepsilon_t$, and $a_t$=random action with probability $\\varepsilon_t$. \n",
        "\n",
        "3. Take action $a_t$, observe reward $r_t$ and next state $s_t'$.\n",
        "\n",
        "4. Add transition $(s_t, a_t, r_t, s_t')$ to the **replay buffer**.\n",
        "\n",
        "4.  Sample a minibatch $\\mathcal{B}$ containing $B$ transitions from the replay buffer. Using this minibatch, we define the loss:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\sum_{(s_i, a_i, r_i, s_i') \\in \\mathcal{B}}\n",
        "\\left[\n",
        "Q(s_i, a_i, \\theta) -  y_i\n",
        "\\right]^2\n",
        "$$\n",
        "where the $y_i$ are the **targets** computed with the **target network** $\\theta^-$:\n",
        "\n",
        "$$\n",
        "y_i = r_i + \\gamma \\max_{a'} Q(s_i', a', \\theta^-).\n",
        "$$\n",
        "\n",
        "5. Update the parameters $\\theta$ to minimize the loss, e.g., with gradient descent (**keeping $\\theta^-$ fixed**): \n",
        "$$\n",
        "\\theta \\gets \\theta - \\eta \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "where $\\eta$ is the optimization learning rate. \n",
        "\n",
        "6. Every $N$ transitions ($t\\mod N$ = 0), update target parameters: $\\theta^- \\gets \\theta$.\n",
        "\n",
        "7. $t \\gets t+1$. Stop if $t = T$, otherwise go to step 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhKHif__t9OD"
      },
      "source": [
        "# Colab setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aylqy_sDqebM",
        "outputId": "31b72afc-18c3-4efd-ab0f-3d94d375dbd2"
      },
      "source": [
        "# After installing, restart the kernel\n",
        "\n",
        "# install rlberry library\n",
        "!git clone https://github.com/rlberry-py/rlberry.git \n",
        "!cd rlberry && git pull && pip install -e .[full] > /dev/null 2>&1\n",
        "!pip install ffmpeg-python > /dev/null 2>&1\n",
        "\n",
        "# packages required to show video\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "# ask to restart runtime\n",
        "print(\"\")\n",
        "print(\" ~~~  Libraries installed, please restart the runtime! ~~~ \")\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rlberry'...\n",
            "remote: Enumerating objects: 190, done.\u001b[K\n",
            "remote: Counting objects: 100% (190/190), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 3259 (delta 93), reused 113 (delta 48), pack-reused 3069\u001b[K\n",
            "Receiving objects: 100% (3259/3259), 834.18 KiB | 6.27 MiB/s, done.\n",
            "Resolving deltas: 100% (2087/2087), done.\n",
            "Already up to date.\n",
            "\n",
            " ~~~  Libraries installed, please restart the runtime! ~~~ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWBRfwosfA9f"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from gym.wrappers import Monitor\n",
        "import gym\n",
        "\n",
        "# Random number generator\n",
        "import rlberry.seeding as seeding \n",
        "rng = seeding.get_rng()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "528oqsgefIFl"
      },
      "source": [
        "# 1. Define the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtExtR4dfMbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4970a39-349e-424e-dccc-bfc5af6c159e"
      },
      "source": [
        "# Environment\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# Discount factor\n",
        "GAMMA = 0.99\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 256\n",
        "# Capacity of the replay buffer\n",
        "BUFFER_CAPACITY = 10000\n",
        "# Update target net every ... episodes\n",
        "UPDATE_TARGET_EVERY = 20\n",
        "\n",
        "# Initial value of epsilon\n",
        "EPSILON_START = 1.0\n",
        "# Parameter to decrease epsilon\n",
        "DECREASE_EPSILON = 200\n",
        "# Minimum value of epislon\n",
        "EPSILON_MIN = 0.05\n",
        "\n",
        "# Number of training episodes\n",
        "N_EPISODES = 400\n",
        "\n",
        "# Learning rate\n",
        "LEARNING_RATE = 0.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Making new env: CartPole-v0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g16Je-dhM2Q"
      },
      "source": [
        "# 2. Define the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvh82br9hMNt"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (state, action, reward, next_state)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return rng.choice(self.memory, batch_size).tolist()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# create instance of replay buffer\n",
        "replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCc9WZppi92W"
      },
      "source": [
        "# 3. Define the neural network architecture, objective and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdNz3Jrwi9iS"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic neural net.\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# Example: \n",
        "\n",
        "# state = env.observation_space.sample()\n",
        "# state_torch = torch.from_numpy(state)\n",
        "\n",
        "# net = Net(state.shape[0], 128, env.action_space.n)\n",
        "# print(net)\n",
        "\n",
        "# q_state = net(state_torch)\n",
        "# print(state)\n",
        "# print(q_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI9hFJ28jLZ_"
      },
      "source": [
        "# create network and target network\n",
        "hidden_size = 128\n",
        "obs_size = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "q_net = Net(obs_size, hidden_size, n_actions)\n",
        "target_net = Net(obs_size, hidden_size, n_actions)\n",
        "\n",
        "# objective and optimizer\n",
        "objective = nn.MSELoss()\n",
        "optimizer = optim.Adam(params=q_net.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnR8nfoSjZjL"
      },
      "source": [
        "# 4. Implement Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6fT8cKdjmTZ"
      },
      "source": [
        "#\n",
        "#  Some useful functions\n",
        "#\n",
        "\n",
        "def get_q(states):\n",
        "    \"\"\"\n",
        "    Compute Q function for a list of states\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        states_v = torch.FloatTensor([states])\n",
        "        output = q_net.forward(states_v).data.numpy()  # shape (1, len(states), n_actions)\n",
        "    return output[0, :, :]  # shape (len(states), n_actions)\n",
        "\n",
        "def eval_dqn(n_sim=5):\n",
        "    \"\"\"\n",
        "    ** TO BE IMPLEMENTED **\n",
        "    \n",
        "    Monte Carlo evaluation of DQN agent.\n",
        "\n",
        "    Repeat n_sim times:\n",
        "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
        "        * Compute the sum of rewards in this episode\n",
        "        * Store the sum of rewards in the episode_rewards array.\n",
        "    \"\"\"\n",
        "    env_copy = deepcopy(env)\n",
        "    episode_rewards = np.zeros(n_sim)\n",
        "\n",
        "    for ii in range(n_sim):\n",
        "        state = env_copy.reset()\n",
        "        done = False \n",
        "        while not done:\n",
        "            action = choose_action(state, 0.0)\n",
        "            next_state, reward, done, _ = env_copy.step(action)\n",
        "            episode_rewards[ii] += reward\n",
        "            state = next_state\n",
        "    return episode_rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMspDNntkIoe"
      },
      "source": [
        "def choose_action(state, epsilon):\n",
        "    \"\"\"\n",
        "    ** TO BE IMPLEMENTED **\n",
        "    \n",
        "    Return action according to an epsilon-greedy exploration policy\n",
        "    \"\"\"\n",
        "    if np.random.uniform() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        q = get_q([state])[0]\n",
        "        return q.argmax()    \n",
        "\n",
        "def update(state, action, reward, next_state, done):\n",
        "    \"\"\"\n",
        "    ** TO BE COMPLETED **\n",
        "    \"\"\"\n",
        "    \n",
        "    # add data to replay buffer\n",
        "    if done:\n",
        "        next_state = None\n",
        "    replay_buffer.push(state, action, reward, next_state)\n",
        "    \n",
        "    if len(replay_buffer) < BATCH_SIZE:\n",
        "        return np.inf\n",
        "    \n",
        "    # get batch\n",
        "    transitions = replay_buffer.sample(BATCH_SIZE)\n",
        "\n",
        "\n",
        "    # values_debug  = torch.zeros(BATCH_SIZE)   # to be computed using batch   ->  Q(s_i, a_i)\n",
        "    # targets_debug = torch.zeros(BATCH_SIZE)   # to be computed using batch   ->  y_i \n",
        "\n",
        "    # for ii, transition in enumerate(transitions):\n",
        "    #     s_i, a_i, r_i, next_s_i = transition\n",
        "\n",
        "    #     # compute Q(s_i, a_i) = Q_i\n",
        "    #     s_i = torch.FloatTensor([s_i])[0]\n",
        "    #     Q_i = q_net(s_i)[a_i]\n",
        "    #     values_debug[ii] = Q_i\n",
        "\n",
        "\n",
        "    #     # compute y_i\n",
        "    #     r_i = torch.FloatTensor([r_i])[0]\n",
        "\n",
        "    #     # \n",
        "    #     max_q_next_state = 0.0\n",
        "    #     if next_s_i is not None:\n",
        "    #         next_s_i = torch.FloatTensor([next_s_i])[0]\n",
        "    #         max_q_next_state = target_net(next_s_i).detach().max()\n",
        "\n",
        "    #     y_i = r_i + GAMMA*max_q_next_state\n",
        "\n",
        "    #     targets_debug[ii] = y_i\n",
        "\n",
        "    # # ----------------------------------------------\n",
        "\n",
        "    # ------------------------\n",
        "    # process batch of (state, action, reward, next_state)\n",
        "    states = [transitions[ii][0] for  ii in range(BATCH_SIZE)]\n",
        "    actions = [transitions[ii][1] for  ii in range(BATCH_SIZE)] \n",
        "    rewards = [transitions[ii][2] for  ii in range(BATCH_SIZE)] \n",
        "\n",
        "    # Attention: next_state is None when the previous state is terminal.\n",
        "    # we handle this using a mask.\n",
        "    next_states = [transitions[ii][3] for  ii in range(BATCH_SIZE) if transitions[ii][3] is not None ] \n",
        "    mask = [transitions[ii][3] is not None for  ii in range(BATCH_SIZE)]\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    states_torch = torch.FloatTensor(states)\n",
        "    actions_torch = torch.LongTensor(actions).view(-1,1)\n",
        "    rewards_torch = torch.FloatTensor(rewards).view(-1, 1)\n",
        "    next_states_torch = torch.FloatTensor(next_states)\n",
        "    mask_torch = torch.BoolTensor(mask)\n",
        "\n",
        "    # Q(s_i, a_i)\n",
        "    values = q_net(states_torch)\n",
        "    values = torch.gather(values, dim=1, index=actions_torch)\n",
        "\n",
        "    # max_a Q(s_{i+1}, a)\n",
        "    values_next_states = torch.zeros(BATCH_SIZE)\n",
        "    values_next_states[mask] = target_net(next_states_torch).max(dim=1)[0].detach()\n",
        "    values_next_states = values_next_states.view(-1, 1)\n",
        "\n",
        "    # targets y_i\n",
        "    targets = rewards_torch + GAMMA*values_next_states\n",
        "    \n",
        "    # Loss function\n",
        "    loss = objective(values, targets)\n",
        "     \n",
        "    # Optimize the model \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    return loss.data.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIhpKPhkkU4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd968965-470f-4ce4-b5db-7667bc20c8c9"
      },
      "source": [
        "\n",
        "#\n",
        "# Train\n",
        "# \n",
        "\n",
        "EVAL_EVERY = 5\n",
        "REWARD_THRESHOLD = 199\n",
        "\n",
        "def train():\n",
        "    state = env.reset()\n",
        "    epsilon = EPSILON_START\n",
        "    ep = 0\n",
        "    total_time = 0\n",
        "    while ep < N_EPISODES:\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # take action and update replay buffer and networks\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        loss = update(state, action, reward, next_state, done)\n",
        "\n",
        "        # update state\n",
        "        state = next_state\n",
        "\n",
        "        # end episode if done\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            ep   += 1\n",
        "            if ( (ep+1)% EVAL_EVERY == 0):\n",
        "                rewards = eval_dqn()\n",
        "                print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n",
        "                if np.mean(rewards) >= REWARD_THRESHOLD:\n",
        "                    break\n",
        "\n",
        "            # update target network\n",
        "            if ep % UPDATE_TARGET_EVERY == 0:\n",
        "                target_net.load_state_dict(q_net.state_dict())\n",
        "            # decrease epsilon\n",
        "            epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN) * \\\n",
        "                            np.exp(-1. * ep / DECREASE_EPSILON )    \n",
        "\n",
        "        total_time += 1\n",
        "\n",
        "# Run the training loop\n",
        "train()\n",
        "\n",
        "# Evaluate the final policy\n",
        "rewards = eval_dqn(20)\n",
        "print(\"\")\n",
        "print(\"mean reward after training = \", np.mean(rewards))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode = 5 , reward =  77.4\n",
            "episode = 10 , reward =  200.0\n",
            "\n",
            "mean reward after training =  200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8QZwuvjgrMm"
      },
      "source": [
        "# Visualize the DQN policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "FGcGwOcEfzPz",
        "outputId": "b50fa57f-8908-45b0-a3ac-7f35796cbdde"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path\n",
        "import base64\n",
        "\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "    html = []\n",
        "    for mp4 in Path(\"videos\").glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay \n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "    \n",
        "env = Monitor(env, './videos', force=True, video_callable=lambda episode: True)\n",
        "\n",
        "for episode in range(1):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    while not done:\n",
        "        action = choose_action(state, 0.0)\n",
        "        state, reward, done, info = env.step(action)\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Clearing 4 monitor files from previous run (because force=True was provided)\n",
            "INFO: Starting new video recorder writing to /content/videos/openaigym.video.3.489.video000000.mp4\n",
            "INFO: Finished writing results. You can upload them to the scoreboard via gym.upload('/content/videos')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"videos/openaigym.video.3.489.video000000.mp4\" autoplay \n",
              "                      loop controls style=\"height: 400px;\">\n",
              "                      <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "                 </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}